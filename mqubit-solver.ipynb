{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # tf 2.x\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "from tensornetwork import ncon\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cmath\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import QGOpt as qgo\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/LuchnikovI/QGOpt\n",
    "    import QGOpt as qgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils as util\n",
    "import channel_utils as c_util\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthis class contains so-called evaluators of type QCEvaluator.\\nEach QCEvaluator is initialized with number of qubits and set of gates it works with (or gate channels)\\nThey contain different tensor network templates, accessible by name.\\nA tensor network template is the array of arrays which gets passed into NCON, defining a TN structure.\\nAn evaluator can evaluate any amplitude of probability obtaining a given bitstring as the output of the circuit.\\nAlso having access to the same probability distribution, evaluator can generate samples for any circuit\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from QCCalc import QCEvaluator\n",
    "'''\n",
    "this class contains so-called evaluators of type QCEvaluator.\n",
    "Each QCEvaluator is initialized with number of qubits and set of gates it works with (or gate channels)\n",
    "They contain different tensor network templates, accessible by name.\n",
    "A tensor network template is the array of arrays which gets passed into NCON, defining a TN structure.\n",
    "An evaluator can evaluate any amplitude of probability obtaining a given bitstring as the output of the circuit.\n",
    "Also having access to the same probability distribution, evaluator can generate samples for any circuit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_dyn(loss_dynamics, num):\n",
    "    plt.plot(loss_dynamics, label = num)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('err')\n",
    "    plt.legend()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hadamard = tf.constant([[1, 1],\n",
    "                        [1, -1]], dtype=tf.complex64) / math.sqrt(2)\n",
    "\n",
    "S = util.matr_Rz(math.pi/2)\n",
    "\n",
    "T = util.matr_Rz(math.pi/4)\n",
    "\n",
    "E = tf.eye(2, dtype=tf.complex64)\n",
    "\n",
    "CZ_44 = tf.constant([[1, 0, 0, 0],\n",
    "                    [0, 1, 0, 0],\n",
    "                    [0, 0, 1, 0],\n",
    "                    [0, 0, 0, -1]], dtype=tf.complex64)\n",
    "\n",
    "E_44 = tf.eye(4, dtype=tf.complex64)\n",
    "\n",
    "CNOT_44 = tf.constant([[1, 0, 0, 0],\n",
    "                    [0, 1, 0, 0],\n",
    "                    [0, 0, 0, 1],\n",
    "                    [0, 0, 1, 0]], dtype=tf.complex64) \n",
    "RNOT_44 = tf.constant([[1, 0, 0, 0],\n",
    "                    [0, 0, 0, 1],\n",
    "                    [0, 0, 1, 0],\n",
    "                    [0, 1, 0, 0]], dtype=tf.complex64)\n",
    "\n",
    "#CNOT = util.convert_44_to_2222(CNOT_44)\n",
    "#CZ = util.convert_44_to_2222(CZ_44)\n",
    "#RNOT = util.convert_44_to_2222(RNOT_44)\n",
    "#big_E = util.convert_44_to_2222(E_44)\n",
    "\n",
    "CNOT = util.swap_legs(tf.reshape(CNOT_44, (2, 2, 2, 2)))\n",
    "CZ = util.swap_legs(tf.reshape(CZ_44, (2, 2, 2, 2)))\n",
    "RNOT = util.swap_legs(tf.reshape(RNOT_44, (2, 2, 2, 2)))\n",
    "big_E = util.swap_legs(tf.reshape(E_44, (2, 2, 2, 2)))\n",
    "\n",
    "H_channel = c_util.convert_1qmatrix_to_channel(Hadamard)\n",
    "S_channel = c_util.convert_1qmatrix_to_channel(S)\n",
    "T_channel = c_util.convert_1qmatrix_to_channel(T)\n",
    "CZ_channel = c_util.convert_2qmatrix_to_channel(CZ)\n",
    "CNOT_channel = c_util.convert_2qmatrix_to_channel(CNOT)\n",
    "big_E_channel = c_util.convert_2qmatrix_to_channel(big_E)\n",
    "RNOT_channel = c_util.convert_2qmatrix_to_channel(RNOT)\n",
    "\n",
    "H_choi = c_util.convert_channel_to_params(H_channel)\n",
    "S_choi = c_util.convert_channel_to_params(S_channel)\n",
    "T_choi = c_util.convert_channel_to_params(T_channel)\n",
    "CZ_choi = c_util.convert_channel_to_params(CZ_channel)\n",
    "CNOT_choi = c_util.convert_channel_to_params(CNOT_channel)\n",
    "big_E_choi = c_util.convert_channel_to_params(big_E_channel)\n",
    "RNOT_choi = c_util.convert_channel_to_params(RNOT_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a \"pure\" set of gates which exist in our mathematical model.\\nThey are considered pure in a sense that they have no noise at all. \\nAny circuits the user runs on his quantum computer must contain only gates described here.\\nHowever, due to imperfectons his version of the same set can be different.\\nFuthermore, in the future versions we should note that each qubit can have its own \"noised\" versions.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_channels_set = {'H':H_channel, 'S':S_channel, 'T':T_channel, 'CZ':CZ_channel}\n",
    "pure_chois_set = {'H':H_choi, 'S':S_choi, 'T':T_choi, 'CZ':CZ_choi}\n",
    "\n",
    "#matr_Rz(math.pi/2.8)[tf.newaxis],\n",
    "#matr_Rz(math.pi/4.1)[tf.newaxis]\n",
    "'''This is a \"pure\" set of gates which exist in our mathematical model.\n",
    "They are considered pure in a sense that they have no noise at all. \n",
    "Any circuits the user runs on his quantum computer must contain only gates described here.\n",
    "However, due to imperfectons his version of the same set can be different.\n",
    "Futhermore, in the future versions we should note that each qubit can have its own \"noised\" versions.''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with Zhenya & Julio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bitstrings_to_file(QC):\n",
    "    a_file = open(\"json/bitstrings.json\", \"w\")\n",
    "    dict_to_export = {}\n",
    "    for name in QC.samples_vault:\n",
    "        dict_to_export[name] = QC.samples_vault[name].numpy().tolist()\n",
    "    json.dump(dict_to_export, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "def import_bitstrings_from_file(QC):\n",
    "    a_file = open(\"json/bitstrings.json\", \"r\")\n",
    "    big_dict = json.load(a_file)\n",
    "    for name in big_dict:\n",
    "        QC.samples_vault[name] = tf.convert_to_tensor(big_dict[name], dtype=tf.int32)\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_templates_to_file(QC):\n",
    "    a_file = open(\"json/templates.json\", \"w\")\n",
    "    dict_to_export = {}\n",
    "    for name in QC.tn_templates:\n",
    "        dict_to_export[name] = QC.tn_templates[name]\n",
    "    json.dump(dict_to_export, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "def import_templates_from_file(QC):\n",
    "    a_file = open(\"json/templates.json\", \"r\")\n",
    "    big_dict = json.load(a_file)\n",
    "    #big_dict = convert_templates_to_ncon(big_dict, QC.n)\n",
    "    for name in big_dict:\n",
    "        QC.tn_templates[name] = big_dict[name]\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_string(string):\n",
    "    parse = string.split(\"_\")\n",
    "    if parse[0] == 'E':\n",
    "        return 4, -1, -1\n",
    "    elif parse[0] == 'H':\n",
    "        return 0, int(parse[1]), -1\n",
    "    elif parse[0] == 'S':\n",
    "        return 1, int(parse[1]), -1\n",
    "    elif parse[0] == 'T':\n",
    "        return 2, int(parse[1]), -1\n",
    "    elif parse[0] == 'CZ':\n",
    "        return 3, int(parse[1]), int(parse[2])\n",
    "    else:\n",
    "        raise 'WRONG FORMAT OF GATE NAME'\n",
    "    \n",
    "def create_tensor_id(gate_type, q1, q2, n):\n",
    "    if (gate_type >= 5) or (gate_type < 0):\n",
    "        raise ('WRONG TENSOR ID')\n",
    "    elif gate_type == 4:\n",
    "        return int(3 * n + n * (n - 1) / 2)\n",
    "    elif gate_type == 3:\n",
    "        if q1 < q2:\n",
    "            return int(3 * n + (q1 - 1) * (2 * n - q1) / 2 + (q2 - q1 - 1))\n",
    "        else:\n",
    "            return int(3 * n + (q2 - 1) * (2 * n - q2) / 2 + (q1 - q2 - 1))\n",
    "    else:\n",
    "        return (n * gate_type + q1 - 1)\n",
    "\n",
    "def convert_templates_to_ncon(tn_templates, n):\n",
    "    new_templates = {}\n",
    "    for name in tn_templates:\n",
    "        tensors, net_struc, con_order, out_order = tn_templates[name]\n",
    "        new_tensors = []\n",
    "        for it in range(len(tensors)):\n",
    "            gate_type, q1, q2 = extract_info_from_string(tensors[it])\n",
    "            #print(tensors[it], gate_type, q1, q2)\n",
    "            new_tensor_id = create_tensor_id(gate_type, q1, q2, n)\n",
    "            new_tensors.append(new_tensor_id)\n",
    "        new_templates[name] = [new_tensors, net_struc, con_order, out_order]\n",
    "    return new_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_dict(target): #maybe needs to be rewritten in case dict mixes its names\n",
    "    \"\"\"\n",
    "    Converts a dict into a long 1D array, dropping the dict names in the process\n",
    "\n",
    "    Args:\n",
    "        target: a python dictionary\n",
    "\n",
    "    Returns:\n",
    "        1D array of elements written one after another\n",
    "        The grouping and names of the groups are lost in the process\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for name in target:\n",
    "        for elem in target[name]:\n",
    "            output.append(elem) #hope it makes a copy\n",
    "    return output\n",
    "\n",
    "def get_complex_channel_form(target): \n",
    "    \"\"\"\n",
    "    Is usually applied to a workspace set of the QuantumCircuits class.\n",
    "    The problems with it are:\n",
    "    1) the set is stored in real form with shape (..., 2)\n",
    "    2) the set has parameter representations A while evaluators work with quantum channels\n",
    "    \n",
    "    Since functions qgo.manifolds.real_to_complex() and convert_params_to_channel() cannot be applied to a dict,\n",
    "    this function breaks the dict into 1D arrays of gates and then converts each array one-by-one\n",
    "    \n",
    "    Args:\n",
    "        target: a python dictionary containing some real-valued parameter representations\n",
    "        of tensors that are in fact complex; shape is (..., 2)\n",
    "\n",
    "    Returns:\n",
    "        The more adequate complex-valued tensors in a dictionary with the same grouping     \n",
    "    \"\"\"\n",
    "    new_set = {}\n",
    "    for name in target:\n",
    "        new_set[name] = c_util.convert_params_to_channel(qgo.manifolds.real_to_complex(target[name]))\n",
    "    return new_set\n",
    "\n",
    "def simplify_sample(sample):\n",
    "    \"\"\"\n",
    "    WORK IN PROGRESS, maybe it's impossible to implement this function at all\n",
    "    the idea is to convert an array of 10000 bitstrings to a dict where \n",
    "    the NAME will be a bitstring [0/1, 0/1, ..., 0/1], and the ELEM will be number of times\n",
    "    the bitstring is encountered in the sample\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for bitstring in sample:\n",
    "        if bitstring not in new_dict:\n",
    "            new_dict[bitstring] = 1\n",
    "        else:\n",
    "            new_dict[bitstring] += 1\n",
    "    return new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuits:\n",
    "    \"\"\"\n",
    "    This is a class which contains everything we need to run the program.\n",
    "    \n",
    "    Attributes:\n",
    "        n: number of qubits the circuit has. Is quite important, since the amount\n",
    "        of 'noisy copies' of each single-qubit gate is equal to number of qubits.\n",
    "    \n",
    "        ideal_set: is a 'pure' set of gate channels which exist in our mathematical model.\n",
    "        The gates are considered pure in a sense that they have no noise at all. \n",
    "        Any circuits the user runs on their quantum computer must contain only gates described here.\n",
    "        However, due to imperfectons user's version of the same set can be different.\n",
    "        Each channel representing a single-qubit gate is a 4x4 complex-valued matrix.\n",
    "        Each channel representing a two-qubit gate is a 4x4x4x4 complex-valued matrix.\n",
    "        \n",
    "        hidden_set: is a set of gate channels the user has in their quantum computer.\n",
    "        They are the \"real\" versions of gates, and can be considered as our final goal when optimizing.\n",
    "        After some iterations of optimizers, we want our \"workspace\" channels to be like the \"real\" versions.\n",
    "        There are n copies of single-qubit gates, and n(n-1)/2 copies of two-qubit gates.\n",
    "        The representation of gate channels is the same as in 'ideal_set'\n",
    "    \n",
    "        workspace_set: is a set of gate channels we actively work with.\n",
    "        These channels are initialized as an 'ideal_set'. \n",
    "        However, iteration-by-iteration they are tuned to be closer and closer to 'real' noised versions.\n",
    "        IMPORTANT: these matrices are real-valued but contain one extra dimension.\n",
    "        So we can perceive them as a 4x4x2/4x4x4x4x2 matrices (last dim is for 'Im' part and 'Re' part)\n",
    "        \n",
    "        tn_templates: is a dictionary of tensor newtork templates which get passed into NCON function.\n",
    "        Each template is accessible by its own name. \n",
    "        These templates are then passed into evaluators (see later).\n",
    "        \n",
    "        samples_vault: is a dictionary of samples which can be generated for each circuit.\n",
    "        A sample is a 2D array of zeros or ones, containing usually 10000 1D arrays - bitsrings.\n",
    "        Each sample is accessible by its own name. In theory, the names in 'samples_vault' \n",
    "        CAN be different from ones used in 'tn_templates', but it is much better to keep them same.\n",
    "        \n",
    "        eval_pure: an evaluator of type QCEvaluator which contains \"pure\" gates ('ideal_set').\n",
    "        Not used at the moment.\n",
    "        \n",
    "        eval_hidden: an evaluator of type QCEvaluator which contains \"real\" gates ('hidden_set').\n",
    "        Used for sample generation.\n",
    "        \n",
    "        eval_workspace: an evaluator of type QCEvaluator which contains \"workspace\" gates ('workspace_set').\n",
    "        Used for calculating the probability distribution, thus obtaining the cost function for the optimizer.\n",
    "        \n",
    "    Functions:\n",
    "        add_circuit: adds a circuit to tn_templates dictionary.\n",
    "        As for now, they DO NOT get immediately passed in evaluators!\n",
    "        \n",
    "        _simple_template: since 'eval_pure' has only four gates, the tn_template must be simplified \n",
    "        before getting passed into evaluator. Remember, set of channels does not have multiple \n",
    "        noised versions. 'eval_pure' will not work with the same template!\n",
    "        \n",
    "        pass_circuits_to_evals: adds each tn_template in 'tn_templates' to all three evaluators. \n",
    "        \n",
    "        init_hidden_channels: The procedure of applying some kind of a noise to simulate user's noised version of gates [demo]\n",
    "        \n",
    "        generate_sample: The procedure of generating N (default=10000) outputs of a circuit defined by 'name'.\n",
    "        Uses 'eval_hidden', so don't forget to pass the circuit in in 'tn_templates' to it!\n",
    "        \n",
    "        generate_all_samples: The procedure of generating N (default=10000) outputs after N runs of all circuits.\n",
    "        \n",
    "        _loss_and_grad: ...\n",
    "        \n",
    "        train_optimizer: Trains an optimizer passed in the function.\n",
    "        Also yields a list of 'loss' values per iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, pure_chois_set, pure_channels_set):\n",
    "        self.n = n\n",
    "        \n",
    "        self.ideal_set = []\n",
    "        for name in pure_channels_set:\n",
    "            self.ideal_set.append(pure_channels_set[name])\n",
    "        \n",
    "        self.hidden_set = None\n",
    "        self.init_hidden_channels(pure_channels_set)\n",
    "        \n",
    "        workspace_set = None\n",
    "        self.init_workspace(pure_chois_set)\n",
    "        \n",
    "        self.tn_templates = {} #label -> list(list1, list2, list3, list4)\n",
    "        \n",
    "        self.samples_vault = {} #label -> tf.tensor(bs, n)\n",
    "        \n",
    "        self.eval_pure = QCEvaluator(self.ideal_set, self.n)\n",
    "        self.eval_hidden = QCEvaluator(unwrap_dict(self.hidden_set), self.n)\n",
    "        self.eval_workspace = QCEvaluator(unwrap_dict(get_complex_channel_form(self.workspace_set)), self.n) #WILL NOT WORK\n",
    "        \n",
    "    def add_circuit(self, tn_template, name):\n",
    "        self.tn_templates[name] = tn_template\n",
    "        #template = [gates_order, ncon]\n",
    "        return\n",
    "    \n",
    "    def _simple_template(self, name): \n",
    "        #need to change gates in the TN template \n",
    "        #we have n H, n S, n T, n(n-1)/2  CZ\n",
    "        #need 1 H, 1 S, 1 T, 1 CZ\n",
    "        #works only for N-S-T-CZ combination\n",
    "        tensors, net_struc, con_order, out_order = self.tn_templates[name]\n",
    "        \n",
    "        new_tensors = tensors.copy()\n",
    "        for i in range(len(new_tensors)):\n",
    "            if (new_tensors[i] == 3 * self.n + self.n * (self.n - 1) / 2):\n",
    "                new_tensors[i] = 4\n",
    "            elif (new_tensors[i] >= 3 * self.n):\n",
    "                new_tensors[i] = 3\n",
    "            else:\n",
    "                new_tensors[i] = int(new_tensors[i] / self.n)\n",
    "        \n",
    "        new_template = [new_tensors, net_struc, con_order, out_order]\n",
    "        return new_template\n",
    "    \n",
    "    def pass_circuits_to_evals(self):\n",
    "        for name in self.tn_templates:\n",
    "            self.eval_pure.add_circuit(self._simple_template(name), name)\n",
    "            self.eval_hidden.add_circuit(self.tn_templates[name], name)\n",
    "            self.eval_workspace.add_circuit(self.tn_templates[name], name)\n",
    "        return\n",
    "        \n",
    "    def init_hidden_channels(self, pure_channels_set):\n",
    "        \"\"\"\n",
    "        Initializes \"true\" versions of noised gates. WIP.\n",
    "        \n",
    "        Args:\n",
    "            pure_channel_set: the \"ideal\" set we change a bit to create noised version.\n",
    "        \n",
    "        Returns:\n",
    "            actually does not return anything, but generates 'hidden_set' for an instance of class.\n",
    "        \"\"\"\n",
    "        self.hidden_set = {}\n",
    "        self.hidden_set['H'] = tf.concat([H_channel[tf.newaxis]] * (self.n - 1) + [tf.eye(4, dtype=tf.complex64)[tf.newaxis]], axis=0)\n",
    "        #self.hidden_set['H'] = tf.concat([H_channel[tf.newaxis]] * self.n, axis=0)\n",
    "        self.hidden_set['S'] = tf.concat([S_channel[tf.newaxis]] * self.n, axis=0)\n",
    "        self.hidden_set['T'] = tf.concat([T_channel[tf.newaxis]] * self.n, axis=0)\n",
    "        self.hidden_set['CZ'] = tf.concat([CZ_channel[tf.newaxis]] * round((self.n * (self.n - 1)) / 2), axis=0)\n",
    "        \n",
    "        #MAKE SOME CRAPPY NOISING SHEIT\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def init_workspace(self, pure_chois_set):\n",
    "        self.workspace_set = {}\n",
    "        self.workspace_set['H'] = tf.Variable(tf.concat(\n",
    "            [qgo.manifolds.complex_to_real(H_choi)[tf.newaxis]] * self.n, axis=0))\n",
    "        self.workspace_set['S'] = tf.Variable(tf.concat(\n",
    "            [qgo.manifolds.complex_to_real(S_choi)[tf.newaxis]] * self.n, axis=0))\n",
    "        self.workspace_set['T'] = tf.Variable(tf.concat(\n",
    "            [qgo.manifolds.complex_to_real(T_choi)[tf.newaxis]] * self.n, axis=0))\n",
    "        self.workspace_set['CZ'] = tf.Variable(tf.concat(\n",
    "            [qgo.manifolds.complex_to_real(CZ_choi)[tf.newaxis]] * round((self.n * (self.n - 1)) / 2), axis=0))\n",
    "        return\n",
    "    \n",
    "    def upd_workspace_eval(self):\n",
    "        self.eval_workspace.gates = unwrap_dict(get_complex_channel_form(self.workspace_set))\n",
    "        return\n",
    "        \n",
    "    def generate_sample(self, name, smpl_size=10000):\n",
    "        \"\"\"\n",
    "        Creates a batch of all-qubit samples for a circuit with name 'name'\n",
    "        using the \"hidden\" evaluator.\n",
    "        \"\"\"\n",
    "        sample = self.eval_hidden.make_full_samples(name, smpl_size)\n",
    "        self.samples_vault[name] = sample\n",
    "        return\n",
    "        \n",
    "    def generate_all_samples(self, smpl_size=10000):\n",
    "        for name in self.tn_templates:\n",
    "            self.generate_sample(name, smpl_size)\n",
    "        return\n",
    "            \n",
    "    #@tf.function\n",
    "    def _loss_and_grad(self, lmbd):\n",
    "        '''\n",
    "        There could be a mistake in calculating the loss function\n",
    "        '''\n",
    "        with tf.GradientTape() as tape:          \n",
    "            channels_dict = get_complex_channel_form(self.workspace_set)\n",
    "            \n",
    "            '''\n",
    "            The workspace_set consists of four tf.Variables. They get unwrapped in a 1D array\n",
    "            and then they get passed into 'eval_workspace'\n",
    "            '''           \n",
    "            \n",
    "            self.eval_workspace.gates = unwrap_dict(channels_dict)\n",
    "   \n",
    "            ''' \n",
    "            NOT USED AT THE MOMENT\n",
    "            #             total_logp = tf.constant(0, dtype=tf.float32)\n",
    "            #             for name in self.tn_templates:\n",
    "            #                 circuit_logp = tf.constant(0, dtype=tf.float32)\n",
    "            #                 simple_sample = simplify_sample(self.samples_vault[name])\n",
    "            #                 for bitstring in simple_sample:\n",
    "            #                     amplitude = self.eval_workspace.evaluate(bitstring, name)\n",
    "            #                     circuit_logp += simple_sample[bitsring] * tf.math.log(amplitude)\n",
    "            #                 total_logp += circuit_logp\n",
    "            #the same shit but different realization \n",
    "            '''   \n",
    "\n",
    "            total_logp = tf.constant(0, dtype=tf.float32)\n",
    "            for name in self.tn_templates: #we iterate by each circuit, the circuit is defined by its name\n",
    "                sample = self.samples_vault[name] #samples are meant to be generated before the optimization\n",
    "                '''\n",
    "                At this point we have 10000 bitstrings\n",
    "                '''\n",
    "                probs = self.eval_workspace.evaluate(sample, name)\n",
    "                '''\n",
    "                At this point we have 10000 probabilities, one for each bitstring\n",
    "                '''\n",
    "                #print(tf.math.abs(probs))\n",
    "                '''\n",
    "                Now we need to sum the logs of each amplitude\n",
    "                '''\n",
    "                circuit_logp = tf.reduce_sum(tf.math.log(tf.math.abs(probs)))\n",
    "                #print(circuit_logp)\n",
    "                total_logp += circuit_logp \n",
    "\n",
    "\n",
    "            total_reg = tf.constant(0, dtype=tf.float32)\n",
    "            for gate_type, gate_id in zip(channels_dict, range(len(self.ideal_set))): #'S', 'H', etc.\n",
    "                gate_type_norm = tf.constant(0, dtype=tf.float32)\n",
    "                for i in range(len(channels_dict[gate_type])): #maybe it broadcasts? no need for a cycle then\n",
    "                    gate_type_norm += tf.math.abs(tf.linalg.norm(channels_dict[gate_type][i] -\\\n",
    "                    (self.ideal_set[gate_id])) ** 2)\n",
    "                total_reg += gate_type_norm\n",
    "            \n",
    "            print('total log', total_logp)\n",
    "            print('total reg', total_reg)\n",
    "            \n",
    "            total_reg *= lmbd\n",
    "        \n",
    "            loss = -total_logp + total_reg\n",
    "    \n",
    "            grad = tape.gradient(loss, self.workspace_set)\n",
    "            #grad = tf.gradients(loss, gate_set_real)\n",
    "        return loss, grad\n",
    "    \n",
    "    def train_optimizer(self, opt, lmbd, iters):\n",
    "        # this list will be filled by value of\n",
    "        # error per iteration\n",
    "        loss_dynamics = []\n",
    "\n",
    "        # optimization loop\n",
    "        for _ in range(iters):\n",
    "            loss, grad = self._loss_and_grad(lmbd)\n",
    "            #print(loss, grad)\n",
    "            \n",
    "            #print(grad)\n",
    "            \n",
    "            # filling list with history of error\n",
    "            loss_dynamics.append(loss)\n",
    "            # optimization step\n",
    "            #for gate_type in self.workspace_set:\n",
    "            opt.apply_gradients(zip(grad.values(), self.workspace_set.values())) #workspace_set is DICT, care!!\n",
    "        return loss_dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_template_5H = [[i for i in range (5)],\n",
    "[[-i, i] for i in range(1,6)],\n",
    "[],\n",
    "[-i for i in range (1, 6)]]  #-i for i in range (1, 51)]\n",
    "\n",
    "GHZ_5_legs = [[i+5, i] for i in range (1, 6)] + [[14+i, 10+i] for i in range (1, 4)] + [[-5, 14]] +\\\n",
    "[[11, -1, 7, 6]] + [[10+i, -i, i+6, i+13] for i in range (2, 5)]\n",
    "\n",
    "tn_template_GHZ_5 = [[0, 1, 2, 3, 4, 1, 2, 3, 4, 15, 19, 22, 24],\n",
    "GHZ_5_legs,\n",
    "[i for i in range (6, 18)],\n",
    "[-i for i in range (1, 6)]]  #-i for i in range (1, 51)]\n",
    "\n",
    "tn_template_microGHZ_5 = [[0, 1, 2, 1, 2, 15, 19, 24],\n",
    "[['a', 1], ['b', 2], [8, 3], [10, 9], [-3, 11], [9, -1, 'b', 'a'], [11, -2, 8, 10], [-5, -4, 5, 4]],\n",
    "['a'] + ['b'] + [i for i in range (8, 12)],\n",
    "[-i for i in range (1, 6)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "QC = QuantumCircuits(5, pure_chois_set, pure_channels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_bitstrings_from_file(QC)\n",
    "import_templates_from_file(QC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QC.add_circuit(tn_template_5H, '5H')\n",
    "#QC.add_circuit(tn_template_GHZ_5, 'GHZ')\n",
    "#QC.add_circuit(tn_template_microGHZ_5, 'microGHZ')\n",
    "\n",
    "QC.pass_circuits_to_evals()\n",
    "#QC.generate_all_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = qgo.manifolds.ChoiMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "lmbd = 25\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-221915.6, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(240.00002, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-53919.652, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(428.8595, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-51121.445, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(425.86758, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-48960.285, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(422.45657, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-47124.918, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(418.70053, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-45479.324, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(414.62955, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-43975.43, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(410.3447, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-42591.14, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(405.98105, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-41316.938, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(401.65875, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-40149.836, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(397.5022, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-39082.03, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(393.65738, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-38110.15, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(390.26587, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-37241.254, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(387.44702, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-36475.016, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(385.29916, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-35799.33, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.87714, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-35210.293, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.16373, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-34709.562, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.07938, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-34290.37, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50848, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-33942.61, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.30777, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-33658.902, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(385.32016, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-33431.836, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.40414, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-33254.457, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(387.44058, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-33118.26, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(388.3272, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-33012.12, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(388.99417, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32929.312, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(389.41757, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32865.54, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(389.6024, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32811.14, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(389.5722, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32758.959, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(389.36978, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32707.248, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(389.04443, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32652.262, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(388.64423, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32590.1, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(388.22086, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32519.758, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(387.8216, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32440.836, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(387.4756, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32354.209, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(387.19672, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32260.248, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.98273, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32160.002, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.8192, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-32056.38, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.69098, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31950.86, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.58124, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31844.32, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.4694, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31739.617, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.33804, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31638.336, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(386.17612, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31541.227, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(385.9794, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31449.334, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(385.7535, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31363.314, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(385.50854, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31283.621, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(385.25372, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31209.814, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.99988, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31141.717, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.7585, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31079.152, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.53873, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-31021.014, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.34735, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30966.387, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.1857, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30914.965, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(384.0514, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30865.627, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.94125, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30817.6, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.85028, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30770.227, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.7734, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30723.115, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.70566, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30675.635, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64288, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30627.6, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.58405, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30578.994, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53018, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30529.555, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48315, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30479.438, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44556, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30428.816, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.41934, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30377.875, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.40674, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30326.781, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.40903, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30275.764, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.42572, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30225.125, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45557, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30174.914, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49585, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30125.494, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5429, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30076.809, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.59332, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-30029.033, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64355, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29982.143, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6908, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29936.248, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.73328, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29891.203, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.77008, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29847.033, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.80087, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29803.695, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.82623, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-29761.053, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.84677, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29719.066, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.86374, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29677.688, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.878, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29636.83, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.89026, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29596.48, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.9008, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29556.541, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.9092, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29517.012, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.91504, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29477.916, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.918, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29439.178, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.91754, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29400.863, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.91333, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29362.938, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.90506, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29325.42, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.89297, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29288.273, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.87762, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29251.535, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.85953, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29215.215, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.8396, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29179.336, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.8186, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29143.752, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.79733, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29108.562, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.77655, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29073.764, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.75677, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29039.299, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.738, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-29005.158, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.7209, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28971.277, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.70526, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28937.744, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.69113, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28904.465, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6785, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28871.434, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.66714, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28838.658, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.65735, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28806.145, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64896, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28773.803, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64203, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28741.76, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6364, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28709.912, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.63226, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28678.3, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.62943, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28646.916, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.62787, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28615.818, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6276, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28584.895, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.62805, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28554.24, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.62927, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28523.816, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.63116, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28493.65, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.63336, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28463.736, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6357, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28434.037, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.63806, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28404.6, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64038, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28375.436, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6424, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28346.445, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64404, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28317.725, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64542, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28289.293, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64642, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28261.059, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6467, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28233.072, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64664, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28205.291, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64612, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28177.744, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.645, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28150.404, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64343, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28123.264, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.64148, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28096.379, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.63892, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28069.697, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6361, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28043.238, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6329, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-28016.98, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6295, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27990.877, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.62592, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27965.004, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.62216, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27939.277, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.6184, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27913.766, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.61456, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27888.516, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.61084, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27863.365, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.60715, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27838.416, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.60358, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27813.666, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.60022, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27789.02, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.59702, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27764.55, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.594, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27740.273, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5912, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27716.2, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.58856, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27692.248, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.58624, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27668.467, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.58408, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27644.83, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.58215, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27621.348, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.58038, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27598.041, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5788, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27574.846, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.57733, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27551.85, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.576, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27528.963, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.57474, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-27506.273, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.57358, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27483.703, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.57245, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27461.285, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.57132, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27439.037, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.57034, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27416.943, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.56912, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27394.938, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.56796, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27373.11, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.56693, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27351.4, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.56567, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27329.887, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.56436, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27308.424, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5631, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27287.146, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.56174, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27266.057, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5603, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27245.043, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.55884, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27224.14, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.55734, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27203.4, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5558, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27182.8, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.55417, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27162.3, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5526, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27141.906, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.551, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27121.672, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.54935, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27101.574, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.54773, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27081.59, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5462, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27061.72, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.54456, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27041.934, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.54297, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27022.297, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.54153, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-27002.8, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53995, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26983.379, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5385, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26964.076, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53723, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26944.91, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53568, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26925.844, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53433, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26906.889, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5332, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26888.004, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53183, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26869.27, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.53055, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26850.648, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5296, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26832.125, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52826, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26813.693, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52707, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26795.371, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52615, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26777.145, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5249, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26759.072, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52377, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26741.053, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52286, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26723.129, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52167, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26705.305, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.52054, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26687.602, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.51962, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26670.041, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5185, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26652.477, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5174, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26635.068, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.51645, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26617.734, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5153, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26600.521, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.51422, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26583.367, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.51328, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26566.328, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.51212, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26549.373, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5111, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26532.504, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.51016, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26515.715, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.509, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26499.041, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.508, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26482.451, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50705, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26465.912, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.5059, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26449.498, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50497, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26433.176, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50403, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26416.951, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50293, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26400.756, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.502, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26384.693, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50116, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26368.723, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.50006, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26352.77, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4992, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26336.979, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49835, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26321.201, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49728, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26305.484, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49658, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26289.88, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4956, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26274.395, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4947, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26258.93, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49405, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26243.568, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.493, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26228.271, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49225, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26213.098, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49155, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26197.916, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.49054, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26182.861, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48987, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26167.902, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48907, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-26152.994, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48822, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26138.182, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48752, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26123.406, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4867, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26108.691, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4859, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26094.08, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4852, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26079.527, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4844, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26065.008, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4836, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26050.639, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48297, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26036.277, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48215, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26022.033, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48138, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-26007.777, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.48083, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25993.678, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47986, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25979.6, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4793, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25965.56, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47864, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25951.64, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47772, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25937.807, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47726, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25923.982, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47653, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25910.24, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4757, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25896.549, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47528, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25882.924, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47452, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25869.377, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4738, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25855.867, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47336, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25842.438, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47253, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25829.104, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47195, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25815.773, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47153, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25802.541, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47058, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25789.398, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.47028, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25776.234, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4696, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25763.16, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4688, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25750.168, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46863, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25737.217, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4677, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25724.336, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4673, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25711.488, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46683, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25698.758, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46597, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25685.984, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4657, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25673.34, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46503, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25660.76, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46442, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25648.197, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46417, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25635.725, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46332, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25623.291, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.463, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25610.879, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46246, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25598.568, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46185, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25586.312, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4615, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25574.023, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.46094, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25561.902, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4604, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25549.781, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4601, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25537.701, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45935, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25525.691, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4591, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25513.729, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45856, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25501.816, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45807, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25489.947, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45776, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25478.184, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4571, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25466.42, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45682, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25454.688, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45633, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25443.04, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45578, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25431.432, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45554, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25419.852, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45496, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25408.367, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45462, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25396.883, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45422, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25385.482, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45383, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25374.139, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45343, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25362.768, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45303, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25351.5, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4526, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25340.293, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45227, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25329.105, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45184, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25317.959, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45157, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25306.89, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45105, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25295.846, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4509, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25284.871, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45032, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25273.904, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45016, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25262.959, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4497, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25252.113, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4494, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25241.291, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4491, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25230.514, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44867, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-25219.777, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4485, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25209.094, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.448, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25198.473, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44788, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25187.86, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44736, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25177.299, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44724, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25166.781, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4468, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25156.309, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44653, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25145.867, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4463, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25135.484, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4459, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25125.13, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44574, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25114.832, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4453, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25104.572, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44522, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25094.346, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44473, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25084.113, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44473, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25073.996, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44418, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25063.846, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44415, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25053.812, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44382, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25043.78, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4435, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25033.744, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44345, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25023.848, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4429, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25013.896, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44305, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-25004.059, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4424, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24994.25, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44257, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24984.398, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44208, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24974.656, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44205, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24964.904, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4417, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24955.262, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44156, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24945.625, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44135, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24935.984, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44116, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24926.438, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4409, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24916.857, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4408, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24907.352, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44052, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24897.953, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44037, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24888.47, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44028, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24879.11, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43994, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24869.715, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43994, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24860.426, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43967, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24851.115, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4395, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24841.855, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43945, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24832.664, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43915, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24823.465, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4391, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24814.334, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43887, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24805.186, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43875, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24796.129, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43863, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24787.06, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43845, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24778.055, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43835, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24769.064, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43817, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24760.1, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43817, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24751.209, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43784, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24742.309, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43808, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24733.492, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43747, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24724.664, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.438, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24715.898, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43707, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24707.117, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4379, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24698.469, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43674, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24689.729, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43793, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24681.105, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43625, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24672.44, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43808, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24663.975, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43567, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24655.332, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43845, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24646.957, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43497, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24638.402, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43896, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24630.168, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43414, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24621.63, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43954, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24613.45, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4336, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24604.885, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43958, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24596.578, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43387, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24588.008, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4383, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24579.666, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43527, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24571.348, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43628, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24563.152, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43704, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24555.074, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4346, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24546.879, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43802, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24538.818, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43414, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-24530.559, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4377, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24522.486, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4349, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24514.344, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4364, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24506.338, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43622, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24498.406, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43506, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24490.379, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4372, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24482.535, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43445, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24474.541, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4373, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24466.701, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43475, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24458.748, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43652, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24450.95, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43567, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24443.146, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43558, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24435.346, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43646, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24427.654, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.435, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24419.863, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43677, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24412.213, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43506, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24404.484, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4365, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24396.852, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43552, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24389.219, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.436, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24381.637, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43604, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24374.08, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43564, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24366.533, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43643, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24359.016, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43552, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24351.547, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43658, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24344.066, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43567, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24336.621, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43652, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24329.25, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43597, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24321.812, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43628, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24314.45, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43643, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24307.162, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43607, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24299.83, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43677, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24292.555, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43607, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24285.234, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43686, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24278.07, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43625, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24270.82, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4368, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24263.645, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4366, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24256.475, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43665, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24249.332, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43692, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24242.215, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43668, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24235.111, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43726, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24228.064, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4367, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24221.012, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4375, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24214.049, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4369, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24207.002, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43753, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24200.041, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4372, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24193.08, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43768, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24186.207, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4374, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24179.24, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43787, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24172.373, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43762, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24165.559, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.438, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24158.672, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4379, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24151.867, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4381, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24145.115, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43814, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24138.31, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43832, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24131.58, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43845, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24124.852, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43845, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24118.14, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4387, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24111.484, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4387, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24104.832, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43893, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24098.193, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43887, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24091.576, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43915, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24084.96, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4392, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24078.408, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43936, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24071.846, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43954, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24065.33, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4396, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24058.82, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43988, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24052.291, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.43988, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24045.85, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44022, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24039.46, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44012, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24033.02, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44055, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24026.611, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44037, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24020.297, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44098, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24013.898, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4405, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-24007.541, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44156, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log tf.Tensor(-24001.266, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44046, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23994.947, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44223, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23988.738, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4404, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23982.426, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44305, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23976.328, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44003, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23970.068, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44418, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23964.086, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4395, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23957.934, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44556, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23952.08, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4389, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23945.826, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44672, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23939.857, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4391, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23933.443, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44638, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23927.223, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.441, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23920.922, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44415, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23914.86, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44406, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23909.033, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44208, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23903.021, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44638, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23897.215, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4416, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23891.059, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44656, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23885.078, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4432, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23879.043, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.445, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23873.145, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44568, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23867.39, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4437, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23861.477, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44714, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23855.629, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44397, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23849.727, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44687, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23843.883, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44553, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23838.078, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44592, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23832.324, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44717, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23826.604, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44547, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23820.863, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4479, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23815.152, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4462, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23809.371, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44757, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23803.686, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44754, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23798.064, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44708, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23792.406, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4486, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23786.781, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44724, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23781.145, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44894, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23775.527, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44806, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23769.975, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44882, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23764.371, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4491, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23758.854, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44882, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23753.322, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4499, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23747.814, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.44925, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23742.293, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45023, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23736.81, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45004, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23731.387, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4504, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23725.871, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45084, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23720.523, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45074, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23715.084, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45142, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23709.727, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45126, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23704.309, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45187, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23698.918, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45187, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23693.6, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.45224, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23688.283, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4525, shape=(), dtype=float32)\n",
      "total log tf.Tensor(-23682.992, shape=(), dtype=float32)\n",
      "total reg tf.Tensor(383.4526, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "opt1 = qgo.optimizers.RAdam(m, lr)\n",
    "loss_dynamics1 = QC.train_optimizer(opt1, lmbd, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_loss_dyn(loss_dynamics1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2223f47e100>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimdim = tf.constant([2,2,2,2,2], dtype=tf.int32)\n",
    "counts = util.tf_ravel_multi_index(QC.samples_vault['5H'], dimdim)\n",
    "hist1 = tf.math.bincount(counts)\n",
    "plt.plot(hist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "difff = (get_complex_channel_form(QC.workspace_set)['H'] - QC.hidden_set['H']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=complex64, numpy=\n",
       "array([0.14394122+0.j, 0.49816415+0.j, 0.1567188 +0.j, 0.02044359+0.j,\n",
       "       2.828424  +0.j], dtype=complex64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.norm(QC.hidden_set['H'] - get_complex_channel_form(QC.workspace_set)['H'], axis = (-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=complex64, numpy=\n",
       "array([0.      +0.j, 0.      +0.j, 0.      +0.j, 0.      +0.j,\n",
       "       2.828427+0.j], dtype=complex64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.norm(QC.hidden_set['H'] - QC.ideal_set[0], axis = (-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=complex64, numpy=\n",
       "array([0.14394122+0.j, 0.49816415+0.j, 0.1567188 +0.j, 0.02044359+0.j,\n",
       "       1.979902  +0.j], dtype=complex64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.norm(QC.ideal_set[0] - get_complex_channel_form(QC.workspace_set)['H'], axis = (-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H': <tf.Tensor: shape=(5, 4, 4), dtype=complex64, numpy=\n",
       " array([[[ 5.5082494e-01+0.j,  4.9741000e-01+0.j,  4.9741000e-01+0.j,\n",
       "           4.4917497e-01+0.j],\n",
       "         [ 4.9741012e-01+0.j, -5.5082488e-01+0.j,  4.4917494e-01+0.j,\n",
       "          -4.9741000e-01+0.j],\n",
       "         [ 4.9741012e-01+0.j,  4.4917494e-01+0.j, -5.5082488e-01+0.j,\n",
       "          -4.9741000e-01+0.j],\n",
       "         [ 4.4917506e-01+0.j, -4.9741009e-01+0.j, -4.9741009e-01+0.j,\n",
       "           5.5082488e-01+0.j]],\n",
       " \n",
       "        [[ 6.2040210e-01+0.j,  4.3121958e-01+0.j,  4.3121958e-01+0.j,\n",
       "           3.0009392e-01+0.j],\n",
       "         [ 4.5486560e-01+0.j, -6.4252323e-01+0.j,  3.1203979e-01+0.j,\n",
       "          -4.5015904e-01+0.j],\n",
       "         [ 4.5486560e-01+0.j,  3.1203979e-01+0.j, -6.4252323e-01+0.j,\n",
       "          -4.5015904e-01+0.j],\n",
       "         [ 3.7959805e-01+0.j, -4.3121952e-01+0.j, -4.3121952e-01+0.j,\n",
       "           6.9990605e-01+0.j]],\n",
       " \n",
       "        [[ 4.5532063e-01+0.j,  4.5089149e-01+0.j,  4.5089149e-01+0.j,\n",
       "           4.4695768e-01+0.j],\n",
       "         [ 4.8153284e-01+0.j, -4.7158524e-01+0.j,  4.7284645e-01+0.j,\n",
       "          -4.7240308e-01+0.j],\n",
       "         [ 4.8153284e-01+0.j,  4.7284645e-01+0.j, -4.7158524e-01+0.j,\n",
       "          -4.7240308e-01+0.j],\n",
       "         [ 5.4467946e-01+0.j, -4.5089152e-01+0.j, -4.5089152e-01+0.j,\n",
       "           5.5304235e-01+0.j]],\n",
       " \n",
       "        [[ 5.0722754e-01+0.j,  4.9994764e-01+0.j,  4.9994764e-01+0.j,\n",
       "           4.9277222e-01+0.j],\n",
       "         [ 4.9994764e-01+0.j, -5.0722766e-01+0.j,  4.9277222e-01+0.j,\n",
       "          -4.9994773e-01+0.j],\n",
       "         [ 4.9994764e-01+0.j,  4.9277222e-01+0.j, -5.0722766e-01+0.j,\n",
       "          -4.9994773e-01+0.j],\n",
       "         [ 4.9277222e-01+0.j, -4.9994773e-01+0.j, -4.9994773e-01+0.j,\n",
       "           5.0722772e-01+0.j]],\n",
       " \n",
       "        [[ 9.9990010e-01+0.j,  9.9970046e-03+0.j,  9.9970046e-03+0.j,\n",
       "           1.0165656e-04+0.j],\n",
       "         [ 9.9970000e-03+0.j, -9.9989778e-01+0.j,  9.9970872e-05+0.j,\n",
       "          -9.9991970e-03+0.j],\n",
       "         [ 9.9970000e-03+0.j,  9.9970872e-05+0.j, -9.9989778e-01+0.j,\n",
       "          -9.9991970e-03+0.j],\n",
       "         [ 9.9950252e-05+0.j, -9.9970037e-03+0.j, -9.9970037e-03+0.j,\n",
       "           9.9989831e-01+0.j]]], dtype=complex64)>,\n",
       " 'S': <tf.Tensor: shape=(5, 4, 4), dtype=complex64, numpy=\n",
       " array([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],\n",
       " \n",
       "        [[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],\n",
       " \n",
       "        [[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],\n",
       " \n",
       "        [[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],\n",
       " \n",
       "        [[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.-1.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]], dtype=complex64)>,\n",
       " 'T': <tf.Tensor: shape=(5, 4, 4), dtype=complex64, numpy=\n",
       " array([[[ 9.9999988e-01+0.0000000e+00j, -4.1096875e-08+4.1096900e-08j,\n",
       "          -4.1096875e-08-4.1096900e-08j,  3.3779341e-15+0.0000000e+00j],\n",
       "         [ 5.8119618e-08+1.0114679e-13j,  7.0710665e-01-7.0710665e-01j,\n",
       "          -2.3885465e-15-2.3884961e-15j, -5.8119777e-08+8.9692170e-14j],\n",
       "         [ 5.8119618e-08-1.0114679e-13j, -2.3885465e-15+2.3884961e-15j,\n",
       "           7.0710665e-01+7.0710665e-01j, -5.8119777e-08-8.9692170e-14j],\n",
       "         [ 3.3779788e-15+0.0000000e+00j,  4.1096882e-08-4.1096900e-08j,\n",
       "           4.1096882e-08+4.1096900e-08j,  1.0000001e+00+0.0000000e+00j]],\n",
       " \n",
       "        [[ 9.9999988e-01+0.0000000e+00j, -4.1096875e-08+4.1096900e-08j,\n",
       "          -4.1096875e-08-4.1096900e-08j,  3.3779341e-15+0.0000000e+00j],\n",
       "         [ 5.8119618e-08+1.0114679e-13j,  7.0710665e-01-7.0710665e-01j,\n",
       "          -2.3885465e-15-2.3884961e-15j, -5.8119777e-08+8.9692170e-14j],\n",
       "         [ 5.8119618e-08-1.0114679e-13j, -2.3885465e-15+2.3884961e-15j,\n",
       "           7.0710665e-01+7.0710665e-01j, -5.8119777e-08-8.9692170e-14j],\n",
       "         [ 3.3779788e-15+0.0000000e+00j,  4.1096882e-08-4.1096900e-08j,\n",
       "           4.1096882e-08+4.1096900e-08j,  1.0000001e+00+0.0000000e+00j]],\n",
       " \n",
       "        [[ 9.9999988e-01+0.0000000e+00j, -4.1096875e-08+4.1096900e-08j,\n",
       "          -4.1096875e-08-4.1096900e-08j,  3.3779341e-15+0.0000000e+00j],\n",
       "         [ 5.8119618e-08+1.0114679e-13j,  7.0710665e-01-7.0710665e-01j,\n",
       "          -2.3885465e-15-2.3884961e-15j, -5.8119777e-08+8.9692170e-14j],\n",
       "         [ 5.8119618e-08-1.0114679e-13j, -2.3885465e-15+2.3884961e-15j,\n",
       "           7.0710665e-01+7.0710665e-01j, -5.8119777e-08-8.9692170e-14j],\n",
       "         [ 3.3779788e-15+0.0000000e+00j,  4.1096882e-08-4.1096900e-08j,\n",
       "           4.1096882e-08+4.1096900e-08j,  1.0000001e+00+0.0000000e+00j]],\n",
       " \n",
       "        [[ 9.9999988e-01+0.0000000e+00j, -4.1096875e-08+4.1096900e-08j,\n",
       "          -4.1096875e-08-4.1096900e-08j,  3.3779341e-15+0.0000000e+00j],\n",
       "         [ 5.8119618e-08+1.0114679e-13j,  7.0710665e-01-7.0710665e-01j,\n",
       "          -2.3885465e-15-2.3884961e-15j, -5.8119777e-08+8.9692170e-14j],\n",
       "         [ 5.8119618e-08-1.0114679e-13j, -2.3885465e-15+2.3884961e-15j,\n",
       "           7.0710665e-01+7.0710665e-01j, -5.8119777e-08-8.9692170e-14j],\n",
       "         [ 3.3779788e-15+0.0000000e+00j,  4.1096882e-08-4.1096900e-08j,\n",
       "           4.1096882e-08+4.1096900e-08j,  1.0000001e+00+0.0000000e+00j]],\n",
       " \n",
       "        [[ 9.9999988e-01+0.0000000e+00j, -4.1096875e-08+4.1096900e-08j,\n",
       "          -4.1096875e-08-4.1096900e-08j,  3.3779341e-15+0.0000000e+00j],\n",
       "         [ 5.8119618e-08+1.0114679e-13j,  7.0710665e-01-7.0710665e-01j,\n",
       "          -2.3885465e-15-2.3884961e-15j, -5.8119777e-08+8.9692170e-14j],\n",
       "         [ 5.8119618e-08-1.0114679e-13j, -2.3885465e-15+2.3884961e-15j,\n",
       "           7.0710665e-01+7.0710665e-01j, -5.8119777e-08-8.9692170e-14j],\n",
       "         [ 3.3779788e-15+0.0000000e+00j,  4.1096882e-08-4.1096900e-08j,\n",
       "           4.1096882e-08+4.1096900e-08j,  1.0000001e+00+0.0000000e+00j]]],\n",
       "       dtype=complex64)>,\n",
       " 'CZ': <tf.Tensor: shape=(10, 4, 4, 4, 4), dtype=complex64, numpy=\n",
       " array([[[[[ 9.94372666e-01+0.j,  7.48039410e-02+0.j,\n",
       "             7.48039410e-02+0.j,  5.62729640e-03+0.j],\n",
       "           [-7.48039410e-02+0.j,  9.94372666e-01+0.j,\n",
       "            -5.62729640e-03+0.j,  7.48039410e-02+0.j],\n",
       "           [-7.48039410e-02+0.j, -5.62729640e-03+0.j,\n",
       "             9.94372666e-01+0.j,  7.48039410e-02+0.j],\n",
       "           [ 5.62729640e-03+0.j, -7.48039410e-02+0.j,\n",
       "            -7.48039410e-02+0.j,  9.94372666e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 3.00480239e-02+0.j, -1.70719475e-01+0.j,\n",
       "            -1.70719475e-01+0.j,  9.69951928e-01+0.j],\n",
       "           [ 1.70719475e-01+0.j,  3.00480351e-02+0.j,\n",
       "            -9.69951928e-01+0.j, -1.70719534e-01+0.j],\n",
       "           [ 1.70719475e-01+0.j, -9.69951928e-01+0.j,\n",
       "             3.00480351e-02+0.j, -1.70719534e-01+0.j],\n",
       "           [ 9.69951928e-01+0.j,  1.70719534e-01+0.j,\n",
       "             1.70719534e-01+0.j,  3.00480444e-02+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 9.96708512e-01+0.j,  5.72772995e-02+0.j,\n",
       "             5.72772995e-02+0.j,  3.29152332e-03+0.j],\n",
       "           [-5.72772995e-02+0.j,  9.96708512e-01+0.j,\n",
       "            -3.29152332e-03+0.j,  5.72772995e-02+0.j],\n",
       "           [-5.72772995e-02+0.j, -3.29152332e-03+0.j,\n",
       "             9.96708512e-01+0.j,  5.72772995e-02+0.j],\n",
       "           [ 3.29152332e-03+0.j, -5.72772995e-02+0.j,\n",
       "            -5.72772995e-02+0.j,  9.96708512e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 3.37264454e-03+0.j, -5.79764545e-02+0.j,\n",
       "            -5.79764545e-02+0.j,  9.96627212e-01+0.j],\n",
       "           [-5.79764545e-02+0.j, -3.37265502e-03+0.j,\n",
       "             9.96627212e-01+0.j,  5.79766333e-02+0.j],\n",
       "           [-5.79764545e-02+0.j,  9.96627212e-01+0.j,\n",
       "            -3.37265502e-03+0.j,  5.79766333e-02+0.j],\n",
       "           [ 9.96627212e-01+0.j,  5.79766333e-02+0.j,\n",
       "             5.79766333e-02+0.j,  3.37266526e-03+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 3.90987806e-02+0.j,  1.93829983e-01+0.j,\n",
       "             1.93829983e-01+0.j,  9.60901141e-01+0.j],\n",
       "           [ 1.93829983e-01+0.j, -3.90987806e-02+0.j,\n",
       "             9.60901141e-01+0.j, -1.93829983e-01+0.j],\n",
       "           [ 1.93829983e-01+0.j,  9.60901141e-01+0.j,\n",
       "            -3.90987806e-02+0.j, -1.93829983e-01+0.j],\n",
       "           [ 9.60901141e-01+0.j, -1.93829983e-01+0.j,\n",
       "            -1.93829983e-01+0.j,  3.90987806e-02+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 9.96627331e-01+0.j,  5.79769276e-02+0.j,\n",
       "             5.79769276e-02+0.j,  3.37269902e-03+0.j],\n",
       "           [-5.79769276e-02+0.j,  9.96627331e-01+0.j,\n",
       "            -3.37269902e-03+0.j,  5.79769276e-02+0.j],\n",
       "           [-5.79769276e-02+0.j, -3.37269902e-03+0.j,\n",
       "             9.96627331e-01+0.j,  5.79769276e-02+0.j],\n",
       "           [ 3.37269902e-03+0.j, -5.79769276e-02+0.j,\n",
       "            -5.79769276e-02+0.j,  9.96627331e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 2.97080316e-02+0.j,  1.69780657e-01+0.j,\n",
       "             1.69780657e-01+0.j,  9.70292211e-01+0.j],\n",
       "           [ 1.69780657e-01+0.j, -2.97080018e-02+0.j,\n",
       "             9.70292211e-01+0.j, -1.69780478e-01+0.j],\n",
       "           [ 1.69780657e-01+0.j,  9.70292211e-01+0.j,\n",
       "            -2.97080018e-02+0.j, -1.69780478e-01+0.j],\n",
       "           [ 9.70292211e-01+0.j, -1.69780478e-01+0.j,\n",
       "            -1.69780478e-01+0.j,  2.97079701e-02+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [ 1.05374269e-08+0.j, -1.05373985e-08+0.j,\n",
       "            -1.05359774e-08+0.j,  1.05360058e-08+0.j],\n",
       "           [ 1.05374269e-08+0.j, -1.05359774e-08+0.j,\n",
       "            -1.05373985e-08+0.j,  1.05360058e-08+0.j],\n",
       "           [ 4.99999970e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  5.00000060e-01+0.j]],\n",
       " \n",
       "          [[ 5.00000060e-01+0.j,  5.00000000e-01+0.j,\n",
       "             5.00000000e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [-2.01543955e-13+0.j,  2.01540417e-13+0.j,\n",
       "            -2.01543941e-13+0.j,  2.01540390e-13+0.j],\n",
       "           [-2.01543955e-13+0.j, -2.01543941e-13+0.j,\n",
       "             2.01540417e-13+0.j,  2.01540390e-13+0.j],\n",
       "           [ 5.00000060e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 9.69950855e-01+0.j,  1.70722663e-01+0.j,\n",
       "             1.70722663e-01+0.j,  3.00491806e-02+0.j],\n",
       "           [ 1.70722663e-01+0.j, -9.69950855e-01+0.j,\n",
       "             3.00491806e-02+0.j, -1.70722663e-01+0.j],\n",
       "           [ 1.70722663e-01+0.j,  3.00491806e-02+0.j,\n",
       "            -9.69950855e-01+0.j, -1.70722663e-01+0.j],\n",
       "           [ 3.00491806e-02+0.j, -1.70722663e-01+0.j,\n",
       "            -1.70722663e-01+0.j,  9.69950855e-01+0.j]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05386633e-08+0.j,\n",
       "             1.05347624e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05347624e-08+0.j,\n",
       "             1.05386633e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [ 5.00000060e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [ 4.00303293e-08+0.j, -4.00303009e-08+0.j,\n",
       "            -2.11622364e-09+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.00303293e-08+0.j, -2.11622364e-09+0.j,\n",
       "            -4.00303009e-08+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.99999970e-01+0.j, -4.99999970e-01+0.j,\n",
       "            -4.99999970e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05386633e-08+0.j,\n",
       "             1.05347624e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05347624e-08+0.j,\n",
       "             1.05386633e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [ 5.00000060e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [ 4.00303293e-08+0.j, -4.00303009e-08+0.j,\n",
       "            -2.11622364e-09+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.00303293e-08+0.j, -2.11622364e-09+0.j,\n",
       "            -4.00303009e-08+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.99999970e-01+0.j, -4.99999970e-01+0.j,\n",
       "            -4.99999970e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]]]],\n",
       " \n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05386633e-08+0.j,\n",
       "             1.05347624e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05347624e-08+0.j,\n",
       "             1.05386633e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [ 5.00000060e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [ 4.00303293e-08+0.j, -4.00303009e-08+0.j,\n",
       "            -2.11622364e-09+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.00303293e-08+0.j, -2.11622364e-09+0.j,\n",
       "            -4.00303009e-08+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.99999970e-01+0.j, -4.99999970e-01+0.j,\n",
       "            -4.99999970e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05386633e-08+0.j,\n",
       "             1.05347624e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05347624e-08+0.j,\n",
       "             1.05386633e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [ 5.00000060e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [ 4.00303293e-08+0.j, -4.00303009e-08+0.j,\n",
       "            -2.11622364e-09+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.00303293e-08+0.j, -2.11622364e-09+0.j,\n",
       "            -4.00303009e-08+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.99999970e-01+0.j, -4.99999970e-01+0.j,\n",
       "            -4.99999970e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 4.99999911e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 4.99999970e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 5.00000060e-01+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j, -1.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "            -1.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  4.99999911e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j, -4.99999970e-01+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  5.00000060e-01+0.j]]],\n",
       " \n",
       " \n",
       "         [[[ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05386633e-08+0.j,\n",
       "             1.05347624e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [-1.05386420e-08+0.j,  1.05347624e-08+0.j,\n",
       "             1.05386633e-08+0.j, -1.05347411e-08+0.j],\n",
       "           [ 5.00000060e-01+0.j, -5.00000000e-01+0.j,\n",
       "            -5.00000000e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 4.99999970e-01+0.j,  4.99999970e-01+0.j,\n",
       "             4.99999970e-01+0.j,  4.99999970e-01+0.j],\n",
       "           [ 4.00303293e-08+0.j, -4.00303009e-08+0.j,\n",
       "            -2.11622364e-09+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.00303293e-08+0.j, -2.11622364e-09+0.j,\n",
       "            -4.00303009e-08+0.j,  2.11625206e-09+0.j],\n",
       "           [ 4.99999970e-01+0.j, -4.99999970e-01+0.j,\n",
       "            -4.99999970e-01+0.j,  4.99999970e-01+0.j]],\n",
       " \n",
       "          [[ 1.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  0.00000000e+00+0.j],\n",
       "           [ 0.00000000e+00+0.j,  0.00000000e+00+0.j,\n",
       "             0.00000000e+00+0.j,  1.00000000e+00+0.j]]]]], dtype=complex64)>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_complex_channel_form(QC.workspace_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=complex64, numpy=\n",
       "array([[ 0.50000006+0.j,  0.50000006+0.j,  0.50000006+0.j,\n",
       "         0.50000006+0.j],\n",
       "       [ 0.50000006+0.j, -0.50000006+0.j,  0.50000006+0.j,\n",
       "        -0.50000006+0.j],\n",
       "       [ 0.50000006+0.j,  0.50000006+0.j, -0.50000006+0.j,\n",
       "        -0.50000006+0.j],\n",
       "       [ 0.50000006+0.j, -0.50000006+0.j, -0.50000006+0.j,\n",
       "         0.50000006+0.j]], dtype=complex64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QC.ideal_set[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
